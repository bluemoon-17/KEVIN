### ✅ 딥러닝 핵심 개념 요약 + 예제 코드

---

#### 🔷 To Categorical (원핫 인코딩)
- 정수형 레이블을 0과 1로 이루어진 벡터로 변환
- 다중 클래스 분류 문제에서 사용
```python
from keras.utils import to_categorical
labels = [0, 1, 2]
one_hot = to_categorical(labels, num_classes=3)
```

---

#### 🔷 Embedding (임베딩)
- 정수 인덱스를 밀집 벡터(1차원 배열)로 매핑
- 자연어 처리에 사용
```python
from keras.layers import Embedding
Embedding(input_dim=1000, output_dim=64, input_length=10)
```

---

#### 🔷 LSTM (Long Short-Term Memory)
- 순환 신경망(RNN)의 일종으로, 시계열 데이터 학습에 유리
- 내부적으로 tanh 활성화 함수 사용
```python
from keras.layers import LSTM
LSTM(units=64, activation='tanh')
```

---

#### 🔷 Dense Layer (완전 연결층)
- 입력과 출력이 모두 연결된 레이어
- 🔹 정의
각 노드가 이전 층의 모든 노드와 연결된 레이어

주로 은닉층이나 출력층에서 사용됨

입력값과 가중치를 곱하고, 바이어스를 더한 후 활성화 함수를 적용
```python
from keras.layers import Dense
Dense(units=10, activation='relu')
```

---

#### 🔷 Softmax (다중 클래스 확률 출력)
🔹 정의
다중 클래스 분류에서 자주 사용되는 활성화 함수

입력값들을 확률로 변환하여 총합이 1이 되도록 정규화

예측 결과를 각 클래스에 대한 확률값으로 해석 가능
```python
Dense(units=3, activation='softmax')
```

---

#### 🔷 손실 함수 (Loss Function)
- 회귀: MSE / 이진분류: binary_crossentropy
- 🔹 정의
모델의 예측값과 실제값 간의 차이를 계산하는 함수

학습 중 오차를 최소화하기 위한 기준으로 사용됨

문제의 유형(회귀 or 분류)에 따라 적절한 손실 함수를 선택
```python
model.compile(loss='mean_squared_error', optimizer='adam')
model.compile(loss='binary_crossentropy', optimizer='adam')
```

---

#### 🔷 Optimizer (최적화 함수)
```python
model.compile(optimizer='adam')
```

---

#### 🔷 EarlyStopping (조기 중단)
  - 조기중단. 성능개선되지않으면 
    오차가 줄지않으면. Patience-몇번 내려가지않으면. 
```python
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', patience=3)
```

---

#### 🔷 Epochs & Batch Size
- Epochs- 성능개성되면 돌리는개수
- 한번에 20개 데이터. Batch-size. 
```python
model.fit(x_train, y_train, epochs=50, batch_size=20)
```

---

#### 🔷 Dropout (과적합 방지)
- 신경망 노트중 일정부분을 꺼주는것. 
학습속도 개선. 과적합 방지. 
```python
from keras.layers import Dropout
Dropout(0.3)
```

---

#### 🔷 Conv1D (1차원 합성곱)CNN사용
🔹 정의
시계열 데이터나 자연어 처리와 같이 1차원 순서형 데이터에서 특징을 추출하는 합성곱 레이어

필터(커널)를 통해 국소 패턴을 감지
🔹 용도
자연어 처리 (문장을 벡터 시퀀스로 변환 후 Conv1D 적용)

주가 예측, 센서 데이터 분석 등
```python
from keras.layers import Conv1D
Conv1D(filters=32, kernel_size=3, activation='relu')
```

---

#### 🔷 MaxPooling1D (특징맵 축소)
- 특징맵 사이즈가 클때. 사이즈를 축소하기위해 사용. Maxpooling - 많이사용하는거
```python
from keras.layers import MaxPooling1D
MaxPooling1D(pool_size=2)
```

---

#### 🔷 Flatten (1차원 배열로 펼침)
- 배열로 변경
```python
from keras.layers import Flatten
Flatten()
```

---

#### 🔷 Sigmoid (이진 분류 활성화 함수)
- 긍정인지 부정인지. - 활성화함수
```python
Dense(units=1, activation='sigmoid')
```

---

#### 🔷 Model Summary (구조 요약)
- 모델을 전체적으로 보여주는거. 
```python
model.summary()
```

---

#### 🔷 커널, 합성곱, 특징맵
- Conv 레이어의 필터 역할, 합성곱 연산을 통해 특징맵 추출
- 커널. 합성곱 연산을 수행하기위한. 가중치가 들어가있는 행렬. 곱해가면서 값을 계산.
- 합성곱을 수행해서 특징맵 추출. 
- 특징맵이 너무 크면 pooling을 통해 특징맵 사이즈를 줄이기. 맥스풀링이 가장많이 사용. 
---

#### 🔷 Padding (입력 가장자리 0으로 채움)
- 0으로 주위를 채움
```python
Conv1D(filters=32, kernel_size=3, padding='same')
```

---

#### 🔷 활성화 함수 vs 손실 함수
- 활성화 함수: relu, sigmoid, softmax, tanh 등
- 손실 함수: mse, binary_crossentropy 등

---

#### 🔷 정규화 (Normalization)
- 0~255 범위의 이미지 데이터를 0~1로 바꾸기위해서 225로 나누는 것
```python
X_train = X_train.astype('float32') / 255.0
```

