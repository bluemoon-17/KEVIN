# CNN(합성곱 신경망)의 이해

### CNN 
1. 0과 1로 이루어진 픽셀의 조합으로 구성
2. 신경망 모델의 입력으로 들어갈 때 1차원 배열로 구성
3. 원래 데이터가 가진 0자 모양의 위상구조 완전히 소멸

### Filter
1. **특징 맵** 생성
2. FIlter의 값은 Input_Data의 특징을 학습하는 **가중치 행렬**
3. 동일한 Filter로 Input_Data 전체에 **합성곱 연산** 적용

### Stride
1. Filter을 적용하기 위해 이동하는 위치의 간격
2. Stride값이 커지면 출력 특징 맵의 **크기 감소**

### Pooling
1. 가로 및 세로 방향으로 **크기를 줄이는 연산**
2. **Pooling Window 및 Stride** 값 지정
3. MaxPooling - 가장 큰 값을 추출하여 하나의 값으로 대체

### Padding
1. 출력크기를 **조정**할 목적으로 사용
2. 합성곱 연산 수행 전에 Input_D 주변을 **0**으로 채움

### Flattem
1. 다차원 배열을 **1차원 배열**로 바꾸는 과정

### Dropout
1. 과적합을 방지하기위한 **정규화 기법**
2. 은닉층에 배치된 노드 중 일부를 꺼줌

# GAN(적대적 생성 신경망)

### GAN
1. 적대적 생성 신경망
2. 가상 이미지를 생성하는 알고리즘
3. ** 이안 굿펠로**가 처음으로 아이디어 제공

## 모델
### 생성 모델
1. 주어진 데이터를 학습히여 **유사한 가상의 데이터**를 생성하는 모델

### 판별 모델
1. 가짜 이미지와 진짜 이미지가 주어졌을 때 이것을 학습하여 ** 진짜와 가짜를 판별**하는 모델

#### 둘다 신경망으로 구분 - 두개가 경쟁하면서 작동

### 정규화
1. 출력분포 유지, 기울기 흐름을 안정화 시킴

### 생성자 코드
1. generator.add(Dense(128*7*7, input_dim=100, activation=**LeekyReLU(0.2)**)) <br>
**LeekyReLU(0.2)** - 입력값이 0보다 작을(음수)경우 0.2를 곱하라<br>

2. generator.add(**BatchNomalization()**) <br>
**BatchNomalization()** - 정규화<br>

3. generator.add(**Reshape((7, 7, 128))**)<br>
**Reshape** - 형태를 바꿔 줌 - 7X7크기의 이미지 모양으로 바꾸면서 채널이 128개인 3차원 텐서로 변환하는 것<br>

0. generator.add(Conv2D(64, **kernal_size=5**, **padding='same'**))<br>
4. **kernal_size=5** - 합성곱 연산시 5X5 커널을 써라<br>
5. **padding='same'** - 패딩 기능을 유지하라<br>


### 판별자 코드
1. discriminator.add(Conv2D(64, kernal_size=5, **strides=2**, input_shape=(28, 28, 1), padding='same'))<br>
**strides=2** - 2셀씩 옮겨서 특징을 추출하라<br>

2.discriminator.add(**Dropout(0.3)**)<br>
**Dropout(0.3)** - 30%의 노드를 꺼라(연산 속도 줄이거나 과적합 방지 위해)<br>

3. discriminator.add(**Flatten()**)<br>
**Flatten()** - 일차원 배열로 바꿔라<br>

4. <8번코드> discriminator.add(Dense(1, **activation='sigmoid'**))<br>
8번코드 - 마지막 출력층 - 출력노드 **1개** - 진위 여부 판별 위해 = **Dense(1, ......)**<br>
5. **activation='sigmoid'** 쓴 이유 - 참과 거짓 판별 위해서
<br>
6. discriminator.**trainable = False** <br>
**trainable = False** - 학습 시키지 마라<br>

# 전이학습

### 전이학습(transfer learning)
1. 기존의 이미지에서 학습한 정보를 가져와 내 프로젝트에 활용하는 것
- 입력데이터가 적을 때 - 전이학습 or GAN 사용 - 과적합 방지

### 전이학습 코드
transfer_model = VGG16(weight='imagenet', include_top=False, imput_shape=(150, 150, 3))<br>
transfer_model.traunable = False<br>
transfer_model.summary()<br>
1. **VGG16(weight='imagenet', include_top=False, imput_shape=(150, 150, 3))**
- VGG16모델을 불러올때 사용하는 방식
- weight='imagennet' - ImageNet 데이터 셋으로 미리 학습된 가중치 사용(전이학습 목적)
- include_top = False - VGG16의 기존 완성된 분류기를 제거하고 특징 추출 부분만 사용
- imput_shape=(150, 150, 3) - 모델에 입력되는 이미지 크기(150X150 RGB 이미지)

# 설명 가능한 인공지능

### 
설명 가능한 인공지능 - AI가 결정을 내린 과정과 이유를 사람이 이해할 수 있도록 설명할 수 있는 AI 시스템


#### 해석성
Interpretability - 전문가가 이해할 수 있는 정도

#### 설명성
Ecplainability - **일반 사용자도 이해할 수 있도록** 의사결정 과정을 설명하는 능력

#### 투명성
Transparency - 모델의 **구조, 파라미터, 학습 과정**이 공개되고 분석 가능함

#### 신성
Reilability - 설명을 통해 AI의 **결정이 일관되고 믿을 수 있는지 확인** 가능함

