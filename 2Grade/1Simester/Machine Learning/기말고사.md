# 결정트리


## 📘 머신러닝 기말고사 정리: Part 1 - 결정트리 (Decision Tree)
### 1. 결정트리 개념 요약
- 개념	설명
- 결정트리란?	데이터를 "질문-답변" 형식으로 나누어 나무 형태로 분류/회귀를 수행하는 모델. 빠르고 직관적이며 전처리가 거의 필요 없음.
- 구성 요소	Root Node (시작 기준), Intermediate Node (중간 분기), Leaf Node (결과)
- 용도	분류(Classification)와 회귀(Regression) 모두 사용 가능

### 2. 결정트리의 작동 방식
데이터를 분할하는 기준을 설정하여 트리를 구성함.

각 분기점(Node)은 데이터 속성을 기준으로 데이터를 분리함.

반복적으로 질문을 던지며 트리를 깊게 확장함 → 과대적합 가능성 있음.

### 3. 과대적합 방지: 가지치기(Pruning)
트리의 깊이를 제한하거나 최소 샘플 수를 설정하여 불필요한 분기를 줄임.

하이퍼파라미터:

max_depth, min_samples_split, min_samples_leaf

### 4. 분할 기준: 불순도 지표
지표	설명	값의 범위	예시
지니 불순도	특정 클래스가 잘 섞였는지를 나타냄. 낮을수록 좋음.	0 (순수) ~ 0.5 (완전히 섞임)	50:50 → 0.5
엔트로피	지니 불순도와 비슷하지만 로그 기반으로 계산됨	0 ~ 1	50:50 → 1

### ✅ 예시: 지니 불순도 계산
plaintext
복사
편집
Gini = 1 - (p₁² + p₂² + ... + pₙ²)
사과 100%: G = 1 - (1²) = 0 (완전 순수)

사과 50%, 바나나 50%: G = 1 - (0.5² + 0.5²) = 0.5

사과 66%, 바나나 33%: G = 1 - ((2/3)² + (1/3)²) ≈ 0.44

### 5. 회귀 트리 (Regression Tree)
회귀 문제에 사용될 수 있음.

각 리프 노드의 예측값은 해당 구간 샘플의 평균.

DecisionTreeRegressor 클래스 사용.

python
복사
편집
from sklearn.tree import DecisionTreeRegressor<br>
<br>
tree_reg = DecisionTreeRegressor(max_depth=2)<br>
tree_reg.fit(X, y)<br>
과대적합 방지: 깊이 제한 or 최소 샘플 수 설정<br>

### 6. 장단점
장점	
직관적이고 해석 쉬움 (화이트박스 모델)	(단) 과대적합 위험 높음
전처리 필요 없음	(단) 데이터 많을수록 느려짐
빠르게 작동	(단) 축 방향(계단형 경계)에 민감

### 7. 화이트박스 vs 블랙박스 모델
모델 유형	예시	설명
화이트박스	결정트리	내부 작동이 명확, 해석 용이
블랙박스	신경망, 랜덤 포레스트	높은 정확도, 해석 어려움

# 앙상블 학습

### 📘 머신러닝 기말고사 정리: Part 2 - 앙상블 학습 (Ensemble Learning)

### #1. 앙상블 학습이란?

여러 개의 **약한 학습기(weak learner)**를 조합해 강한 모델을 만드는 방식.

대표 기법: 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)

단일 모델보다 일반화 성능이 우수.

### 2. 배깅과 페이스팅

구분

배깅(Bagging)

페이스팅(Pasting)

샘플링

중복 허용(bootstrap)

중복 없이 샘플링

대표 클래스

BaggingClassifier

BaggingClassifier (with bootstrap=False)

- 장점

과대적합 감소, 다양성 증가

메모리 절약, 속도 빠름

OOB 평가 (Out-of-Bag)

일부 데이터가 훈련에 사용되지 않음 → 검증 세트로 활용 가능

oob_score=True 설정 시 자동 평가

### 3. 랜덤 포레스트 (Random Forest)

여러 개의 결정 트리를 배깅 방식으로 학습시킨 모델

각 트리는 무작위로 특성의 일부만 사용하여 훈련

- 장점:

과대적합 감소

특성 중요도 확인 가능 → feature_importances_

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
print(rf.feature_importances_)

### 4. 부스팅 (Boosting)

이전 모델이 틀린 샘플에 더 집중하여 학습을 반복함

종류:

AdaBoost: 오답에 가중치 ↑

Gradient Boosting: 이전 예측의 잔차에 새 모델을 적합시킴

from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier(n_estimators=100)
ada.fit(X_train, y_train)

장점: 높은 정확도

단점: 느릴 수 있음, 이상치에 민감

### 5. 스태킹 (Stacking)

서로 다른 여러 모델의 예측을 기반으로 메타 모델이 최종 예측

VotingClassifier는 다수결(hard voting), 평균(soft voting) 방식

StackingClassifier는 예측값 자체를 입력으로 받아 학습함
<br>
from sklearn.ensemble import StackingClassifier<br>
from sklearn.linear_model import LogisticRegression<br>
from sklearn.svm import SVC<br>
from sklearn.tree import DecisionTreeClassifier<br>
<br>
base_models = [<br>
    ('svc', SVC(probability=True)),<br>
    ('dt', DecisionTreeClassifier())<br>
]<br>
meta_model = LogisticRegression()<br>
stack_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)<br>
stack_model.fit(X_train, y_train)<br>

| 기법    | 방식         | 장점             | 단점                 |
|---------|--------------|------------------|----------------------|
| 배깅    | 병렬 학습    | 과적합 감소      | 전체 모델 크기 큼    |
| 부스팅  | 순차 학습    | 예측 정확도 높음 | 이상치에 민감, 느림  |
| 스태킹  | 메타 학습기 사용 | 성능 향상        | 구조 복잡, 튜닝 필요 |

#### 앙상블 기법은 정확도 향상 및 과적합 방지에 매우 유용함.

#### 시험 포인트: 각 기법의 차이점, 구현 방법, 주요 파라미터 기억하기!## 📉 머신러닝 기말고사 정리: Part 3 - 차원 축소 (Dimensionality Reduction)

# 차원축소

### 1. 개념 요약

고차원 데이터 → 저차원으로 변환

차원의 저주(Curse of Dimensionality) 해결 목적

고차원일수록 계산량 증가, 과적합 위험 증가

### 2. 주성분 분석 (PCA)

고차원 데이터에서 분산이 가장 큰 방향으로 축을 찾아 투영

선형 변환 기반 기법

주요 속성:

explained_variance_ratio_: 각 주성분의 분산 비율

n_components: 차원 수 설정

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

### 3. 점진적 PCA (Incremental PCA)

대규모 데이터셋을 위한 미니배치 기반 PCA

from sklearn.decomposition import IncrementalPCA
ipca = IncrementalPCA(n_components=2, batch_size=100)
X_reduced = ipca.fit_transform(X)

### 4. 랜덤 투영 (Random Projection)

확률적으로 차원을 축소하는 방식

GaussianRandomProjection, SparseRandomProjection 제공

### 5. LLE (Locally Linear Embedding)

비선형 차원 축소 기법

국지적 선형 관계 유지하며 매니폴드를 펼침

스위스 롤 데이터 같은 비선형 구조에 유리

### 6. 기타 기법 요약

| 기법   | 특징                           |
|--------|--------------------------------|
| MDS    | 샘플 간 거리 보존 중심          |
| Isomap | 지오데식 거리 기반, 그래프 사용 |
| t-SNE  | 비슷한 샘플은 가깝게, 다른 건 멀리 배치 |
| LDA    | 선형 분류 기반, 클래스 간 거리 최대화 |

### ✅ 정리 요약

PCA: 선형 차원 축소, 분산 최대 방향 투영

LLE / Isomap / t-SNE: 비선형 구조 보존

IncrementalPCA: 대용량 데이터에 효과적

차원 축소는 시각화, 과적합 방지, 전처리에 유용함

시험 대비: PCA 작동 원리, explained_variance_ratio_, 랜덤 투영 특징 기억!

# 비지도 학습 & 군집

### 🧠 머신러닝 기말고사 정리: Part 4 - 비지도 학습 & 군집 (Unsupervised Learning & Clustering)

### 1. 비지도 학습이란?

정답(레이블) 없이 데이터 간 유사성을 바탕으로 구조를 발견하는 학습

주요 목적: 군집화, 이상치 탐지, 밀도 추정 등

### 2. K-평균 (K-Means)

데이터 포인트를 k개의 클러스터로 묶는 알고리즘

각 클러스터 중심(centroid)까지 거리 기준으로 샘플을 할당

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
labels = kmeans.labels_

장점: 단순하고 빠름

단점: k를 미리 알아야 함, 비구형 클러스터에 취약

### 3. 클러스터 수 선택 기준

엘보우 방법: inertia(군집 내 거리 제곱합) 그래프의 꺾이는 지점

실루엣 점수: 군집 간 거리와 군집 내 거리의 비율을 사용

### 4. DBSCAN (Density-Based Spatial Clustering)

밀도가 높은 구역을 클러스터로 지정

ε-이웃 내 샘플 수가 min_samples 이상이면 핵심 샘플

클러스터 수를 지정할 필요 없음, 이상치 감지 가능

from sklearn.cluster import DBSCAN<br>
db = DBSCAN(eps=0.5, min_samples=5)<br>
labels = db.fit_predict(X)<br>
<br>
labels_, core_sample_indices_ 확인 가능<br>
<br>
predict() 없음 → 새로운 샘플 예측은 불가<br>

### 5. 군집 응용

이미지 분할: RGB 색상값을 기반으로 세그먼트 분할

준지도 학습: 군집 중심에 수동 라벨 부여 → 전체 데이터로 전파

능동 학습: 모델이 불확실한 샘플만 선택하여 라벨 요청

### ✅ 정리 요약

| 알고리즘 | 특징       | 장점                     | 단점                         |
|----------|------------|--------------------------|------------------------------|
| K-평균   | 중심점 기준 | 빠르고 단순              | k 사전 설정 필요, 원형 클러스터에만 적합 |
| DBSCAN   | 밀도 기반   | 이상치 탐지 가능, 클러스터 수 지정 불필요 | 희소 데이터에 취약, 파라미터 민감        |

- 비지도 학습은 탐색적 데이터 분석, 전처리, 이상치 감지에 널리 활용됨
- 시험 대비: 각 군집 알고리즘의 동작 방식, 장단점 비교 기억하기!

# 코드 예제

### 🧪 머신러닝 기말고사 정리: Part 5 - 코드 예제 & 실습 설명

### 1. 결정트리 분류기

from sklearn.tree import DecisionTreeClassifier<br>
clf = DecisionTreeClassifier(criterion='gini', max_depth=3)<br>
clf.fit(X_train, y_train)<br>
y_pred = clf.predict(X_test)<br>
<br>
criterion: 분할 기준 (gini 또는 entropy)<br>
<br>
max_depth: 과적합 방지용 트리 최대 깊이 설정

### 2. 랜덤 포레스트 중요도 출력

from sklearn.ensemble import RandomForestClassifier<br>
clf = RandomForestClassifier()<br>
clf.fit(X_train, y_train)<br>
print(clf.feature_importances_)<br>

각 특성이 예측에 얼마나 영향을 주는지 분석

### 3. PCA 사용 예시

from sklearn.decomposition import PCA<br>
pca = PCA(n_components=2)<br>
X_reduced = pca.fit_transform(X)<br>

데이터의 주요 특성 두 개로 차원 축소

### 4. KMeans 군집화

from sklearn.cluster import KMeans<br>
kmeans = KMeans(n_clusters=3)<br>
kmeans.fit(X)<br>
labels = kmeans.labels_<br>

n_clusters: 클러스터 수 지정

### 5. DBSCAN 군집화

from sklearn.cluster import DBSCAN<br>
db = DBSCAN(eps=0.5, min_samples=5)<br>
labels = db.fit_predict(X)<br>

eps: 밀도 기준 반경

min_samples: 최소 샘플 수

### 6. 앙상블 - 스태킹 예제

from sklearn.ensemble import StackingClassifier<br>
from sklearn.linear_model import LogisticRegression<br>
from sklearn.svm import SVC<br>
from sklearn.tree import DecisionTreeClassifier<br>
<br>
base_models = [<br>
    ('svc', SVC(probability=True)),<br>
    ('dt', DecisionTreeClassifier())<br>
]<br>
meta_model = LogisticRegression()<br>
stack_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)<br>
stack_model.fit(X_train, y_train)<br>

여러 모델을 조합하여 최종 예측 정확도 향상

### ✅ 정리 요약

실습 문제는 Scikit-learn 기본 클래스를 정확히 알고 있는지 묻는 경우가 많음

각 모델별 주요 매개변수, .fit(), .predict() 기본 흐름 파악 중요

시험 대비: 각 코드의 목적, 매개변수, 출력 결과 해석 가능해야 함

