## 📝 10주차: 객체 인식 및 이미지 분할 상세 요약 (시험 대비)

### 1. 객체 인식 (Object Detection)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **객체 인식 정의** | **정의** | 이미지 내 객체에 대해 **분류**하는 문제와 객체의 **위치 검출(Localization)**을 박스(Bounding Box)로 나타내는 문제를 동시에 다루는 분야입니다. |
| **분류** | **1단계 vs 2단계** | 딥러닝 객체 인식 알고리즘은 크게 **2단계 객체 인식(2-stage detector)**과 **1단계 객체 인식(1-stage detector)**으로 나눌 수 있습니다.  |
| **2단계 인식** | **Faster R-CNN 계열** | R-CNN → Fast R-CNN → **Faster R-CNN** 순으로 발전했습니다. <br> 후보 영역(Region Proposal)을 먼저 찾은 후, 해당 영역에 대해 분류를 수행합니다. |
| | **Faster R-CNN** | **후보 영역 추출 네트워크 (RPN, Region Proposal Network)**를 도입하여, 후보 영역 추출 과정까지 신경망 내부에서 자동화했습니다. 이는 Selective Search 등 외부 알고리즘을 사용했던 이전 모델들의 단점을 개선했습니다. |
| | **Fast R-CNN** | R-CNN의 속도 문제를 개선하기 위해 **RoI 풀링(Region of Interest Pooling)**을 도입했습니다. 이미지를 CNN에 한 번만 넣어 Feature Map을 만든 뒤, RoI 풀링을 통해 Feature Map 위에서 여러 영역(RoI)을 처리합니다. |
| **1단계 인식** | **SSD (Single Shot MultiBox Detector)** | 후보 영역을 찾는 과정 없이, 한 번의 연산으로 객체의 위치와 분류를 탐지합니다. **속도가 빠릅니다.** |
| | **SSD 특징** | 크기와 비율이 다른 **앵커 박스(Anchor Box)**를 미리 설정하고, CNN의 여러 Feature Map(다양한 해상도)을 활용하여 **Multi-scale 탐지**를 수행합니다. (고해상도 Feature Map은 작은 객체, 저해상도 Feature Map은 큰 객체 탐지에 유리). |

---

### 2. 이미지 분할 (Image Segmentation)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **이미지 분할 정의** | **정의** | 이미지의 모든 픽셀을 분류하여 각 픽셀이 어떤 카테고리에 속하는지 파악하는 기술입니다 (Semantic Segmentation). |
| **FCN** | **Fully Convolutional Network** | 신경망의 모든 계층을 Convolution과 Pooling으로만 구성하여, 최종 출력을 이미지와 같은 크기로 만듭니다. 입력 이미지 크기에 제약이 없습니다. |
| **U-Net** | **특징** | FCN 기반이며, U자 형태의 구조를 가집니다. <br> **인코더(수축 경로)**와 **디코더(확장 경로)**를 **스킵 커넥션(Skip Connection)**으로 연결하여 인코더에서 손실된 위치 정보를 디코더로 전달해 정밀한 분할을 수행합니다.  |
| **PSPNet** | **Pyramid Scene Parsing Network** | CVPR 2017에서 발표된 모델입니다. <br> **피라미드 풀링 모듈 (PPM)**을 추가하여 이미지 전체를 큰 그림부터 작은 부분까지 다양한 크기의 **시야(Receptive Field)**로 바라봅니다. |
| | **PPM 핵심 아이디어** | Feature Map을 여러 크기로 **요약(Pooling)**하여 **전역 정보**와 **지역 정보**를 모두 확보한 뒤, 이를 다시 합쳐(Concatenate) 장면 전체의 의미를 정확하게 파악합니다. |


## 📝 11주차: 시계열 분석 상세 요약 (시험 대비)

### 1. 시계열 분석 기초 및 통계 기반 모델

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **시계열 자료** | **정의** | 시간의 흐름에 따라 수집된 데이터입니다 (예: 주가, 일별 기온). <br> 추세를 파악하거나 향후 전망 등을 **예측**하기 위한 용도로 사용됩니다. |
| **4대 구성 요소** | **구성** | 1. **불규칙 변동** (패턴이 없어 모형화 어려움) 2. **추세 변동** (장기적인 변화 추세) 3. **순환 변동** 4. **계절 변동** |
| | **불규칙 변동** | 예측 불가능한 돌발적 요인으로 발생하며, 모형을 만들기 매우 어렵습니다 (예: 자연재해, 일시적 이벤트). |
| **통계 모델** | **AR (자기회귀 모델)** | 현재 값이 **과거 자신의 값**들의 선형 결합으로 설명된다는 가정. ("이전 관측값이 이후 관측값에 영향을 준다"). |
| | **MA (이동평균 모델)** | 현재 값이 **과거 예측 오차(충격)**의 선형 결합으로 설명된다는 가정. |
| | **ARMA** | AR 모델과 MA 모델을 결합한 모델입니다. |
| | **ARIMA** | **AR** (자기회귀), **I** (차분, Integrated), **MA** (이동평균)의 약자. <br> **차분(Differencing)**을 적용하여 **비정상 시계열** (추세가 있는 데이터)을 **정상 시계열**로 변환한 후, ARMA 구조를 적용합니다. |

### 2. 딥러닝 기반 시계열 모델

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **RNN (순환 신경망)** | **정의** | 순서가 있는 시퀀스 데이터(시계열, 문장 등)를 처리하기 위한 신경망. <br> 이전 시점의 정보를 **은닉 상태(Hidden State)**에 담아 다음 시점에 입력하여 순서를 '기억'합니다.  |
| | **문제점** | 장기적으로 거리가 먼 시점의 정보가 사라지는 **장기 의존성(Long-Term Dependency) 문제**와 **기울기 소실(Vanishing Gradient)** 문제가 발생합니다. |
| **LSTM** | **Long Short-Term Memory** | RNN의 장기 의존성 문제를 해결하기 위해 설계되었습니다. <br> **셀 상태(Cell State)**와 **게이트** (Input, Forget, Output Gate)를 사용해 정보를 장기간 보존하거나 버릴 수 있습니다.  |
| **GRU** | **Gated Recurrent Unit** | LSTM보다 단순화된 모델로, **Update Gate**와 **Reset Gate** 단 2개의 게이트만 사용합니다. 셀 상태와 은닉 상태를 통합하여 사용합니다. <br> LSTM과 비슷한 성능을 보이며 연산량이 적어 효율적입니다. |
| **양방향 RNN** | **Bidirectional RNN** | **과거의 정보(Forward)**와 **미래의 정보(Backward)**를 동시에 사용하여 출력 값을 예측합니다. <br> 현재 단어를 예측할 때, 지나온 단어뿐만 아니라 이후 단어의 문맥까지 모두 활용하여 의미 해석 **정확도를 향상**합니다 (예: 문장에서 '은행'의 의미 파악). |


## 📝 12주차: 자연어 처리 및 전처리 상세 요약 (시험 대비)

### 1. 자연어 처리 (Natural Language Processing, NLP)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **자연어 처리 정의** | **정의** | 우리가 일상생활에서 사용하는 언어(자연어)의 의미를 분석하여 **컴퓨터가 처리할 수 있도록** 하는 과정입니다. |
| **분야의 특징** | **난이도** | 딥러닝에 대한 이해뿐만 아니라 **인간 언어에 대한 이해**도 필요하기 때문에 접근하기 어려운 분야입니다. <br> 또한, 언어 종류(영어, 중국어, 한국어 등)가 다르고 그 형태가 다양하여 처리가 매우 어렵습니다. |
| **처리 완성도** | **완성도 높은 영역** | 스팸 처리(spam detection), 맞춤법 검사(spell checking), 단어 검색(keyword search), 개체명 인식(named entity recognition) 등은 이미 완성도가 높습니다. |
| | **발전이 필요한 영역** | 질의응답(question & answering), 대화(dialogue) 등은 아직 발전이 더 필요한 분야입니다.  |
| **예시 (중국어)** | **띄어쓰기 문제** | 영어는 명확한 띄어쓰기가 있지만, 중국어는 띄어쓰기가 없기 때문에 **단어 단위의 임베딩**이나 처리가 어렵습니다. |

### 2. 자연어 전처리 (Preprocessing)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **전처리 정의** | **정의** | 컴퓨터가 자연어를 학습하기 전에 원본 데이터를 정제하고 가공하는 과정입니다. |
| **필요성** | **이유** | 잡음을 제거하고, 데이터의 특징을 부각하여 모델의 성능을 높이기 위해 필수적입니다. |
| **토큰화** | **Tokenization** | 문장을 의미 있는 단위인 **토큰(Token)**으로 나누는 과정입니다. <br> 토큰은 단어, 형태소, 또는 서브워드(Subword) 등이 될 수 있습니다. |
| | **단어 토큰화** | 띄어쓰기나 구두점을 기준으로 단어 단위로 분리합니다 (가장 단순한 방법). |
| | **문장 토큰화** | 마침표(.), 물음표(?) 등 문장의 경계를 기준으로 분리합니다. |
| **정제 (Cleaning)** | **노이즈 제거** | 불필요한 HTML 태그, 특수문자 등을 제거합니다. |
| | **정규화 (Normalization)** | 같은 의미를 가진 단어들을 동일하게 통일합니다 (예: 'good'과 'goood'을 'good'으로 통일). |
| **품사 태깅** | **POS (Part-of-Speech) Tagging** | 단어에 **품사(명사, 동사, 형용사 등)** 정보를 부여하는 과정입니다. <br> 이는 문맥에 따라 단어의 의미를 정확히 파악하는 데 중요합니다 (예: '날다'와 '배'). |
| **불용어 제거** | **Stopword Removal** | 문장의 의미에 큰 영향을 미치지 않지만 자주 출현하는 단어(예: 'a', 'the', '은/는/이/가')를 제거합니다. |
| **표제어 추출/어간 추출** | **표제어 추출 (Lemmatization)** | 단어의 형태가 바뀌어도 **원형(Lemma)**을 찾습니다 (예: 'am', 'are', 'is' → 'be'). |
| | **어간 추출 (Stemming)** | 단어의 **어간(Stem)**, 즉 변하지 않는 부분을 추출합니다 (정확도는 표제어 추출보다 낮을 수 있음). |
| **자연어 처리 라이브러리** | **예시** | **NLTK**, **KoNLPy** (한국어 형태소 분석 라이브러리) 등이 있습니다. <br> *참고: KoNLPy를 사용하기 위해서는 Java(Oracle JDK) 설치 및 환경 설정이 필요합니다.*  |


## 📝 13주차: 자연어 처리를 위한 임베딩 및 Transformer 상세 요약

### 1. 임베딩 (Embedding)

| 구분 | 주요 개념 | 상세 설명 |
| :--- | :--- | :--- |
| **임베딩 정의** | **정의** | 사람이 사용하는 언어(자연어)를 컴퓨터가 이해할 수 있는 언어(숫자) 형태인 **벡터(vector)**로 변환한 결과 또는 일련의 과정을 의미합니다. |
| **임베딩 역할** | **3가지 역할** | ① 단어 및 문장 간 **관련성 계산** ② **의미적** 혹은 **문법적 정보의 함축** (예: '왕' - '남자' + '여자' = '여왕') ③ 텍스트 데이터의 **밀집 표현** 구현 |
| **임베딩 유형** | **4가지 유형** | ① 희소 표현 기반 임베딩 ② 횟수 기반 임베딩 ③ 예측 기반 임베딩 ④ 횟수/예측 기반 임베딩 |

### 2. 임베딩 방법론

| 유형 | 방법론 | 특징 및 장단점 |
| :--- | :--- | :--- |
| **① 희소 표현 기반** | **원-핫 인코딩 (One-Hot Encoding)** | 단어 N개를 각각 N차원의 벡터로 변환 (해당 단어 위치만 1, 나머지는 0). <br> **단점**: 단어 간 **유사도 파악 불가능**, 단어 수만큼 차원이 늘어나는 **'차원의 저주'** 문제 발생. |
| **② 횟수 기반** | **Count Vectorizer** | 단어의 단순한 **출현 빈도수**를 이용해 벡터를 생성합니다. |
| | **TF-IDF** | **단어 빈도(TF)**와 **역문서 빈도(IDF)**를 곱하여 단어의 중요도에 가중치를 부여합니다. 자주 등장하는 불필요한 단어의 가중치를 낮춰 효과적이며, 검색 엔진 등에 사용됩니다. |
| **③ 예측 기반** | **Word2Vec** | **신경망 학습**을 통해 단어의 의미적/문법적 정보를 내포하는 **밀집 벡터(Dense Vector)**를 생성합니다. |
| | **CBOW** (Continuous Bag Of Words) | **주변 단어(문맥)**들을 보고 **중심 단어**를 예측합니다. |
| | **Skip-gram** | **중심 단어**를 보고 **주변 단어(문맥)**들을 예측합니다. (일반적으로 CBOW보다 성능이 좋음) |
| | **FastText** | 단어를 **N-그램** 단위로 임베딩하여, 희소 단어나 오탈자 등에도 강한 임베딩을 수행합니다. |
| **④ 횟수/예측 기반** | **GloVe** (Global Vectors for Word Representation) | Word2Vec의 로컬 통계(문맥) 정보와 TF-IDF의 전역 통계(전체 코퍼스) 정보를 모두 활용하여 임베딩합니다. |

### 3. Transformer

| 구분 | 주요 개념 | 상세 설명 |
| :--- | :--- | :--- |
| **Transformer** | **특징** | 2017년 논문(Attention Is All You Need)에서 발표되었습니다. <br> **순환 구조(RNN/LSTM)**를 완전히 제거하고 **어텐션(Attention) 메커니즘**만을 이용해 문맥 의존성을 계산합니다. |
| **핵심 기술** | **어텐션 (Attention)** | 모든 벡터 중에서 꼭 살펴보아야 할 벡터들에 **집중**하겠다는 의미입니다. 문장의 의미를 파악하는 핵심 요소입니다. |
| **구조** | **인코더-디코더** | 기본적인 Sequence-to-Sequence 구조와 동일하게 **인코더**와 **디코더**로 구성되어 있습니다. |
| | **인코더** | 입력 문장을 받아 문맥 정보를 추출합니다. (총 N개 계층으로 구성). |
| | **디코더** | 인코더의 출력 정보를 받아 번역된 문장 등 최종 결과를 출력합니다. (총 N개 계층으로 구성). |
| **한국어 임베딩** | **처리** | 한국어 임베딩 과정도 영어와 기본적으로 **동일**합니다. 텍스트 생성 후 토큰화하고, 이를 바탕으로 인코딩을 진행합니다. (파일 내에서 한국어 임베딩의 예시 코드를 제시함). |


## 📝 14주차: 생성 모델 상세 요약 (시험 대비)

### 1. 생성 모델 기초

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **생성 모델 정의** | **정의** | 데이터의 **분포(Distribution)**를 학습한 후, 그 분포에서 **새로운 샘플** (예: 이미지, 텍스트)을 만들어내는 모델입니다. <br> **분류 모델**은 진짜와 가짜 데이터를 분류하는 것이 주 목적입니다. |
| **대표 모델** | **종류** | GMM (Gaussian Mixture Model), HMM (은닉 마르코프 모델), **오토인코더(AE)**, **변분 오토인코더(VAE)**, **GAN (Generative Adversarial Network)**, Diffusion Model 등이 있습니다. |

### 2. 오토인코더 (Autoencoder, AE)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **정의** | **Autoencoder** | 입력된 데이터에서 주요 특징을 뽑아 **압축**하였다가 다시 원래대로 **복원**하는 신경망입니다. |
| **구조** | **인코더-디코더** | **인코더(Encoder)**: 입력 데이터를 압축하여 **잠재 벡터(latent vector)**를 생성합니다. <br> **디코더(Decoder)**: 잠재 벡터를 받아 다시 원래의 입력 데이터 형태로 복원합니다. |
| **활용** | **사용 목적** | 데이터의 **차원 축소**, 주요 **특징 추출** 등에 사용됩니다. |

### 3. 변분 오토인코더 (Variational Autoencoder, VAE)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **정의** | **VAE** | 일반 오토인코더의 잠재 벡터(latent vector) 공간에 **확률 분포(가우시안 분포)**를 가정하고 적용한 모델입니다. |
| **생성 과정** | **샘플링** | 인코더는 잠재 벡터 $z$를 바로 출력하지 않고, **평균($\mu$)**과 **분산($\sigma^2$)**의 두 벡터를 출력합니다. <br> 이 분포에서 **랜덤하게 샘플링**하여 잠재 벡터 $z$를 얻은 후 디코더에 입력합니다. |
| **특징** | **다양성 확보** | 단순 복원만 하는 AE와 달리, 잠재 공간에서 임의의 점을 샘플링하여 입력 데이터와 유사하지만 **다양한 형태**의 새로운 데이터를 생성할 수 있습니다. |

### 4. 생성적 적대 신경망 (Generative Adversarial Network, GAN)

| 구분 | 주요 개념 | 상세 설명 및 특징 |
| :--- | :--- | :--- |
| **GAN 정의** | **정의** | **생성자(Generator)**와 **판별자(Discriminator)**라는 두 모델이 서로 **적대적(Adversarial)**으로 경쟁하며 학습하는 신경망입니다.  |
| **생성자 (Generator)** | **역할** | 무작위 노이즈(Noise)를 입력받아, 최대한 **진짜와 구별되지 않는** 가짜 데이터를 생성하려고 합니다. |
| **판별자 (Discriminator)** | **역할** | 진짜 데이터와 생성자가 만든 가짜 데이터를 입력받아, 입력된 데이터가 **진짜인지 가짜인지 판별**합니다. |
| **학습 원리** | **경쟁** | 생성자는 판별자를 속이려고 학습하고, 판별자는 생성자가 만든 가짜를 정확히 구별하려고 학습합니다. 이 경쟁을 통해 두 모델 모두 성능이 향상됩니다. |
| **손실 함수** | **Loss Function** | 생성자와 판별자 모두 **(이진) 크로스 엔트로피(BinaryCrossentropy)**를 손실 함수로 사용합니다. |
| **옵티마이저** | **Optimizer** | 학습 시 생성자와 판별자 모두 **아담(Adam) 옵티마이저**를 사용하는 예시가 제시되었습니다. |
