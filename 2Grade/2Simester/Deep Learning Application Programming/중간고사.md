## 💻 딥러닝 핵심 개념 정리 (2주차 ~ 7주차 전체 요약)

### 2주차: 인공지능 기초 및 데이터 정의

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **인공지능 계층** | **딥러닝 (DL)** | **정의:** **심층 신경망(Deep Neural Network)**을 사용하여 복잡한 패턴을 학습하는 머신러닝 알고리즘. (AI ⊃ ML ⊃ DL) |
| **인공지능 계층** | **머신러닝 (ML)** | **정의:** 데이터를 기반으로 **스스로 학습**하여 성능을 반복적으로 개선해나가는 컴퓨터 알고리즘. |
| **학습 유형** | **지도 학습** | **의미:** 입력 데이터($X$)와 **정답 레이블($Y$)**을 모두 사용하여 모델을 훈련시키는 방식. |
| **학습 유형** | **비지도 학습** | **의미:** 정답(레이블) 없이 입력 데이터($X$)만으로 데이터의 **내재된 구조나 패턴**을 스스로 발견하여 학습하는 방식. |
| **데이터 변수** | **독립 변수 (Independent Variable)** | **정의:** 모델의 **입력값**으로 사용되며 종속 변수에 영향을 주는 변수. |

---

### 3주차: 텐서, 데이터 전처리 및 머신러닝 알고리즘 (K-NN, SVM)

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **데이터 구조** | **텐서 (Tensor)** | **정의:** 딥러닝에서 데이터를 표현하는 기본 단위로, **3차원 이상의 배열** 형태를 가짐. |
| **딥러닝 프레임워크** | **텐서플로 (TensorFlow)** | **정의:** 데이터 흐름 그래프를 사용하여 **텐서의 수치 연산을 수행**하는 오픈 소스 프레임워크. |
| **데이터 전처리** | **원-핫 인코딩** | **목적:** **범주형 데이터를 숫자 벡터**로 변환하여 신경망이 처리할 수 있게 하는 방식. |
| **지도 학습** | **K-NN (K-최근접 이웃)** | **정의:** **게으른 학습자(Lazy Learner)** 기반. 새로운 데이터와 **가장 가까운 K개 이웃**의 정보를 바탕으로 예측하는 알고리즘. |
| **지도 학습** | **SVM (서포트 벡터 머신)** | **정의:** 두 클래스 사이의 **마진(거리)을 최대화**하는 **초평면**을 찾아 분류를 수행하는 알고리즘. |

---

### 4주차: 머신러닝 핵심 알고리즘 (Decision Tree, Regression)

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **지도 학습** | **결정 트리 (Decision Tree)** | **정의:** 데이터를 여러 단계의 질문(조건)으로 분기하여 최종적으로 분류 또는 예측을 수행하는 **트리 구조** 모델. |
| **결정 트리** | **정보 이득 (Information Gain)** | **정의:** 노드를 분할하기 전후의 **불순도 감소량**. 이 값이 **최대**가 되도록 노드 분할 기준을 선택합니다. |
| **지도 학습** | **로지스틱 회귀** | **정의:** **분류** 모델. 선형 회귀 결과를 **시그모이드 함수**에 적용하여 출력값을 **0과 1 사이의 확률**로 변환해 이진 분류를 수행. |
| **지도 학습** | **선형 회귀 (Linear Regression)** | **정의:** **회귀** 모델. 독립 변수와 종속 변수 간의 관계를 **최적의 직선**으로 모델링하여 연속적인 값을 예측하고 설명. |

---

### 5주차: 딥러닝 기본 요소와 최적화 및 비지도 학습 알고리즘

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **신경망 요소** | **활성화 함수** | **의미:** 뉴런의 출력 신호를 결정하고, 딥러닝 모델에 **비선형성**을 도입하여 복잡한 함수 근사를 가능하게 함. |
| **신경망 요소** | **손실 함수 (Loss Function)** | **정의:** 모델의 **예측값과 실제 정답값 사이의 오차**를 계산하는 함수. 학습 목표는 이 값을 최소화하는 것. |
| **최적화 알고리즘** | **경사 하강법** | **목적:** 손실 함수를 최소화하는 **최적의 가중치**를 찾기 위해, **기울기 반대 방향**으로 가중치를 반복적으로 갱신하는 알고리즘. |
| **최적화 기법** | **모멘텀 (Momentum)** | **의미:** 경사 하강법에 **관성** 개념을 도입하여 학습 중 발생하는 지역 최솟값에 갇히는 것을 방지하고 수렴 속도를 높이는 기법. |
| **최적화 기법** | **NAG (Nesterov Accelerated Gradient)** | **의미:** 모멘텀 적용 후 **미리 이동한 지점**에서 기울기를 재계산하여 더 정확하게 최적점을 탐색하는 기법. |
| **비지도 학습** | **K-means Clustering** | **정의:** 데이터를 **K개의 군집**으로 묶는 군집화 알고리즘. 클러스터 중심점과의 거리를 최소화하는 방향으로 중심점을 갱신. |
| **비지도 학습** | **PCA (Principal Component Analysis)** | **목적:** 데이터의 **차원 축소** 기법. 데이터의 **분산이 가장 큰 방향(주성분)**을 찾아 정보 손실을 최소화하며 데이터를 압축함. |

---

### 6주차: 합성곱 신경망 (CNN)의 구성 요소

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **CNN의 필요성** | **지역적 특징 추출** | **의미:** 이미지 전체가 아닌 작은 부분을 계산하여 **자원 소모를 줄이고**, 이미지의 엣지, 질감 등 **지역적 특징**을 효율적으로 학습하기 위해 CNN을 사용. |
| **CNN 구조** | **합성곱 층 (Convolutional Layer)** | **정의:** 입력 이미지에 **필터(Filter)**를 적용하여 **특징 맵(Feature Map)**을 추출하는 층. |
| **CNN 구조** | **필터 (Filter / Kernel)** | **정의:** 합성곱 층에서 사용되며 이미지의 **지역적 특징을 학습**하는 작은 행렬(가중치). |
| **CNN 구조** | **풀링 층 (Pooling Layer)** | **목적:** 특징 맵의 **크기(차원)를 줄여** 계산량을 감소시키고, 미세한 위치 변화에 모델이 덜 민감하도록 **강인성**을 부여하는 층. |
| **CNN 응용** | **GCN (Graph Convolutional Network)** | **정의:** **그래프 구조 데이터**(노드와 에지)를 처리하기 위한 신경망. 인접 행렬 등으로 변환하여 노드와 주변 노드의 정보를 함께 학습함. |

---

### 7주차: LeNet-5 및 CNN 심화 구조

| 주제 | 핵심 개념 | 설명 (정의/의미) |
| :--- | :--- | :--- |
| **최초의 CNN** | **LeNet-5** | **정의:** 1995년 **얀 르쿤**이 개발한 **최초의 CNN 구조**. **합성곱(C)과 다운 샘플링(S)**의 반복 구조를 정립함. |
| **CNN 구조 특징** | **완전연결층 (Fully Connected Layer)** | **의미:** 합성곱/풀링 층에서 추출된 **모든 특징**을 받아 최종적으로 **분류(Classification)**를 수행하는 신경망 층. |
| **CNN 심화** | **잔차 학습 (Residual Learning)** | **목적:** 네트워크 층이 깊어질 때 발생하는 **성능 저하** 문제를 해결하기 위해 **숏컷(Skip Connection)**을 도입하여 학습을 용이하게 하는 방식 (ResNet의 핵심). |
| **CNN 심화** | **숏컷 / 스킵 커넥션** | **정의:** ResNet에서 사용되며, 입력 $x$를 한 층 또는 블록의 출력에 **바로 더해주는** 연결 경로. 잔차 학습을 가능하게 하여 역전파 시 기울기 소실을 방지함. |




## 💻 딥러닝 응용 프로그래밍 핵심 개념 코드 분석 (2주차 ~ 7주차)

# ==============================================================================
# 2주차: 인공지능 기초 및 데이터 정의
# ==============================================================================

# AI, ML, DL 계층 구조
HIERARCHY = "AI ⊃ ML ⊃ DL" # # 주석: 딥러닝은 심층 신경망을 사용하는 머신러닝의 하위 집합

# 데이터 변수 정의
X = "독립 변수 (Independent Variable)" # # 주석: 모델의 입력값 (원인)
Y = "종속 변수 (Dependent Variable)"   # # 주석: 예측하려는 목표 변수 (결과)

# 학습 유형 구분
LEARNING_TYPE_SUPERVISED = "X와 Y(정답)를 모두 사용" # # 주석: 지도 학습 (Supervised Learning)
LEARNING_TYPE_UNSUPERVISED = "X만 사용하여 패턴 발견" # # 주석: 비지도 학습 (Unsupervised Learning)

# ==============================================================================
# 3주차: 텐서, 데이터 전처리 및 ML 알고리즘 (K-NN, SVM)
# ==============================================================================

# 텐서 (Tensor)
DATA_STRUCTURE = "3차원 이상의 배열" # # 주석: 딥러닝에서 데이터는 텐서 형태로 표현됨

# 원-핫 인코딩 (One-Hot Encoding)
def one_hot_encode(category_list):
    vector = [0] * len(category_list)
    # # 주석: 범주형 데이터를 신경망이 처리 가능한 숫자 벡터로 변환 (해당 위치에만 1 부여)
    vector[category_list.index(category)] = 1
    return vector

# SVM (Support Vector Machine)의 목표
SVM_GOAL = "Maximize Margin" # # 주석: 초평면과 가장 가까운 데이터(서포트 벡터) 사이의 마진을 최대화

# ==============================================================================
# 4주차: 머신러닝 핵심 알고리즘 (Decision Tree, Regression)
# ==============================================================================

# 결정 트리 (Decision Tree) 노드 분할 기준
def calculate_information_gain(parent_impurity, children_impurity_avg):
    # # 주석: 노드 분할 시, 불순도(Impurity) 감소량인 정보 이득이 최대가 되도록 분할
    return parent_impurity - children_impurity_avg 

# 로지스틱 회귀 (Logistic Regression) 모델
# # 주석: 선형 회귀 결과를 Sigmoid 함수에 적용하여 0~1 사이의 확률로 변환 (분류 모델)
LOGISTIC_MODEL = "P(Y=1) = 1 / (1 + exp(-(W*x + b)))" 

# 선형 회귀 (Linear Regression) 모델
LINEAR_MODEL = "y = W * x + b" # # 주석: 독립변수(x)를 사용하여 종속변수(y)를 예측하는 최적의 직선

# ==============================================================================
# 5주차: 딥러닝 기본 요소와 최적화 및 비지도 학습 (PCA)
# ==============================================================================

# 경사 하강법 (Gradient Descent) - 가중치 갱신 공식
def update_weights_SGD(W_old, LR, gradient):
    # # 주석: 손실 함수 기울기(gradient)의 반대 방향으로 학습률(LR)만큼 가중치 갱신
    W_new = W_old - LR * gradient
    return W_new

# NAG (Nesterov Accelerated Gradient) 옵티마이저 원리
# # 주석: 모멘텀이 적용된 '미리 이동한 지점'에서 기울기를 계산하여 모멘텀의 단점(과도한 이동) 개선
NAG_PRINCIPLE = "Update_at(W + Momentum_direction) using new_gradient"

# PCA (Principal Component Analysis)
# # 주석: 데이터의 차원 축소 기법. 분산이 가장 큰 방향(주성분)을 찾아 데이터를 압축
PCA_GOAL = "Find Principal Components (Max Variance) for Dimensionality Reduction"

# ==============================================================================
# 6주차: 합성곱 신경망 (CNN)의 구성 요소
# ==============================================================================

# 합성곱 층 (Convolutional Layer)의 출력 크기 계산 공식
def calculate_output_size(H_in, F, P, S):
    # # 주석: 필터(F) 적용 후 특징 맵의 크기 계산 (H_in: 입력 크기, P: 패딩, S: 스트라이드)
    H_out = (H_in - F + 2 * P) // S + 1 
    return H_out

# 풀링 층 (Pooling Layer)의 목적
POOLING_GOAL = "Down-sampling" # # 주석: 특징 맵의 차원을 줄여 계산량 감소 및 모델의 강인성 확보

# GCN (Graph Convolutional Network)의 입력 데이터 변환
# # 주석: 그래프 구조 데이터를 신경망이 처리하도록 인접 행렬(Adjacency Matrix)로 변환
GCN_INPUT_TRANSFORM = "Graph -> Adjacency Matrix (인접 행렬)"

# ==============================================================================
# 7주차: LeNet-5 및 CNN 심화 (ResNet)
# ==============================================================================

# LeNet-5 구조의 기본 원리
# # 주석: CNN의 초석이 된 구조. 합성곱(C)과 다운 샘플링/풀링(S)을 반복하여 계층적으로 특징 추출
LE_NET_5_STRUCTURE = "C -> S -> C -> S -> F(완전연결)"

# ResNet (Residual Network) - 잔차 학습 블록 구조
def residual_block(x):
    F_x = layers(x)
    # # 주석: 숏컷(Skip Connection)을 통해 입력(x)을 잔차(F_x)에 더해 잔차 학습 수행
    H_x = F_x + x 
    return H_x
