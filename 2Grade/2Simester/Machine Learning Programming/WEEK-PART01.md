# 강화학습, 마르코프 과정 및 몬테카를로 방법 요약

## 1. 강화 학습의 기본 (Reinforcement Learning Fundamentals)

### 1.1. 강화 학습의 정의 및 특징
* **정의**: 컴퓨터가 스스로 **행동의 방식**을 익히는 기계 학습(Machine Learning)의 한 분야입니다.
* **원리**: **보상(Reward)**을 통해 상은 최대화하고 벌은 최소화하는 방향으로 행위를 강화하는 학습입니다.
* **분류**: 지도 학습(Supervised Learning)이나 비지도 학습(Unsupervised Learning)과 달리, 정답을 가르쳐주지 않고 보상을 통해 최적 행동을 찾습니다.
* **역사적 모티브**: 하버드 대학의 스키너 교수가 1931년에 제시한 **스키너 상자(Skinner Box)** 실험에서 모티브를 얻었습니다.

### 1.2. 강화 학습의 기본 요소
| 요소 | 정의 |
| :--- | :--- |
| **에이전트 (Agent)** | 강화 학습을 실행하는 주체로, 주어진 문제 상황에서 행동하는 주체입니다 (예: 게임 캐릭터, 로봇). |
| **환경 (Environment)** | 에이전트가 상호작용하는 대상이며, 에이전트의 행동을 입력으로 받아 처리하고 **보상(Reward)**과 **다음 관찰 정보(Next Observation)**를 반환합니다.
| **상태 (State, S)** | 환경으로부터 받는 관찰 정보나, 행동, 보상 등으로 구성된 기록 정보를 통해 에이전트 스스로 관리하는 환경의 상태 정보입니다.

### 1.3. 환경의 종류
* **에피소딕 환경 (Episodic Environment)**: 종료 상태가 존재하며, 시작 상태에서 종료 상태에 도달할 때까지의 과정을 **에피소드(Episode)** 단위로 고려합니다.
* **지속적 환경 (Continuing Environment)**: 종료가 없는, 지속적인 상태를 다룹니다 (예: 주식 가격 변화).

---

## 2. 마르코프 과정 및 정책과 가치 함수 (Markov Process, Policy & Value Function)

### 2.1. 마르코프 과정 (Markov Process / Markov Chain)
* **정의**: 시간에 따라 주어진 환경의 상태 변화를 **상태 전이 확률(State Transition Probability)**로 기술하는 확률 과정입니다.
* **마르코프 속성 (Markov Property)**: 임의의 타임 스텝 $t$에서의 상태 $S_t$는 바로 직전의 상태 $S_{t-1}$에만 의존하여 결정된다는 가정입니다. 즉, 과거의 모든 상태는 현재 상태를 통해 요약됩니다.
    $$\text{수식:} \quad P(S_t = s' | S_{t-1} = s, S_{t-2} = s_{t-2}, \dots) = P(S_t = s' | S_{t-1} = s)$$

* **구성 요소**: 마르코프 프로세스는 상태 집합 $S$와 상태 전이 확률 $p(s'|s)$ 및 초기 상태 확률 $\mu$를 구성 요소로 지닌 $<S, p, \mu>$로 구성됩니다.

### 2.2. 정책 (Policy)
* **정의**: 에이전트가 주어진 상태 $s$에서 어떠한 행동 $a$를 취할 것인지에 대한 확률적 혹은 결정적 규칙을 의미합니다.
* **표기**: $\pi(a|s)$ (상태 $s$에서 행동 $a$를 선택할 확률)

### 2.3. 가치 함수 (Value Function)
* **정의**: 현재 상태에서 정책 $\pi$를 따랐을 때 얻게 될 **미래 보상의 총합**에 대한 기댓값을 나타냅니다.
* **상태 가치 함수 ($V_{\pi}(s)$)**: 특정 상태 $s$에 있을 때 정책 $\pi$를 따를 경우 얻는 미래 보상의 기댓값입니다.
    $$V_{\pi}(s) = E_{\pi}[G_t | S_t = s]$$
* **행동 가치 함수 ($Q_{\pi}(s, a)$)**: 특정 상태 $s$에서 특정 행동 $a$를 취한 후 정책 $\pi$를 따를 경우 얻는 미래 보상의 기댓값입니다.
    $$Q_{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a]$$

### 2.4. 벨만 방정식 (Bellman Equation)
* **개념**: 가치 함수는 현재 상태의 가치와 다음 상태의 가치 간의 재귀적인 관계를 나타내는 **벨만 방정식**을 통해 표현 및 계산됩니다.

### 2.5. 최적 가치 함수와 최적 정책
* **최적 상태 가치 함수 ($V^{*}(s)$)**: 모든 가능한 정책 중 최대의 상태 가치를 가지는 함수입니다.
* **최적 행동 가치 함수 ($Q^{*}(s, a)$)**: 모든 가능한 정책 중 최대의 행동 가치를 가지는 함수입니다.
* **최적 정책 ($\pi^{*}$)**: 최적 가치 함수를 달성하게 하는 정책입니다. 최적 행동 가치 함수 $Q^{*}(s, a)$를 얻을 수 있다면, **탐욕적(Greedy) 방법**을 통해 결정적 최적 정책을 얻을 수 있습니다.
    $$\text{결정적 최적 정책:} \quad \pi^{*}(s) = \arg\max_a Q^{*}(s, a)$$

---

## 3. 몬테카를로 예측 (Monte Carlo Prediction)

### 3.1. 몬테카를로 방법의 개요
* **정의**: 모나코의 카지노 도시 몬테카를로에서 이름이 유래되었으며, 알아내고자 하는 값을 **수많은 반복적 시행 및 관찰**을 통해 **확률적으로 근사**하여 구하는 방법입니다.
* **강화 학습에서의 적용**: 환경에 대한 **완벽한 지식(상태 전이 확률 $p$)이 필요하지 않을 때** 사용됩니다. 환경과의 상호작용을 통해 얻는 **경험 샘플(상태, 행동, 보상)**을 사용하여 가치 함수와 최적 정책을 갱신합니다.
* **제한**: **종료 상태가 존재하는 에피소딕 환경**에서만 적용 가능하며, 타임 스텝 사이에서는 정책 개선 수행이 불가능하고 에피소드와 에피소드 사이에서만 정책 개선이 이루어집니다.

### 3.2. 몬테카를로 예측 (Monte Carlo Prediction)
* **목적**: 임의의 정책 $\pi$를 따라 생성되는 여러 에피소드에 걸쳐서 상태 $s$에 여러 번 방문한 후, 해당 정책 $\pi$의 상태 가치 $V_{\pi}(s)$ 또는 행동 가치 $Q_{\pi}(s, a)$를 **경험적 평균**을 통해 추정하는 것입니다.
* **상태 가치 ($V_{\pi}(s)$) 추정**: 
    $$V_{\pi}(s) \approx \frac{1}{N}\sum_{i=1}^{N}G_{i}$$
    * $N$: 상태 $s$를 방문한 총 횟수.
    * $G_i$: $i$번째 상태 $s$의 방문으로부터의 **감가 이득(Return)** (미래 보상의 총합).

### 3.3. 몬테카를로 예측의 두 가지 방법
1.  **모든 방문 MC 예측 (Every-visit MC Prediction)**: 동일한 에피소드 내에서 상태 $s$를 **매번 방문할 때마다** 그 이후의 모든 보상의 합의 평균을 활용하여 $V_{\pi}(s)$를 추정합니다.
2.  **첫 방문 MC 예측 (First-visit MC Prediction)**: 동일한 에피소드 내에서 상태 $s$를 **처음 방문했을 때에만** 그 이후의 모든 보상들의 합의 평균을 활용하여 $V_{\pi}(s)$를 추정합니다.

---

## 4. 정책과 가치 산출 실험 (GridWorld 예시)

### 4.1. 실험 환경 및 목표
* **환경**: **그리드 월드(GridWorld)** 환경에서 상태 가치, 최적 상태 가치, 최적 행동 가치 및 정책을 산출하는 실험을 진행합니다.
* **예시**: **웜홀(Wormhole)**이 있는 그리드 월드 환경을 설정하여 특정 상태(A, B)에서는 어떤 행동을 해도 지정된 다음 상태($A'$, $B'$)로 이동하며 특별한 보상(+10, +5)을 받는 상황을 가정합니다.

### 4.2. 주요 알고리즘 및 코드 구성
* **상태 가치 산출**: 각 상태별 상태 가치를 산출하며, **벨만 방정식**을 활용하여 다음 상태 전이에 대한 가치를 계산하고 갱신합니다.
* **최적 가치 산출**: 최적 상태 가치 함수 $V^{*}(s)$를 찾기 위해 **벨만 최적 방정식**을 활용하여 현재와 갱신된 값의 비교를 통해 최적값을 찾습니다.
* **최적 정책 산출**: 최적 행동 가치 함수 $Q^{*}(s, a)$의 벨만 최적 방정식을 활용하여 최적 정책을 산출합니다.

---
