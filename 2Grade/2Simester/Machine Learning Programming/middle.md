말씀하신 대로, 시험 공부용으로 강의 자료를 요약해 드리겠습니다.

다만 저는 PDF의 text만 볼 수 있어서, 펜으로 표시하시거나 밑줄 그으신 부분을 시각적으로 알 수는 없습니다. 대신, 시험에 중요하게 다뤄질 **핵심 개념과 정의, 그리고 알고리즘 간의 비교** 위주로 전체 내용을 요약해 드렸습니다.

### 📝 1. 시험 정보 (Page 2)

* **시험 유형**
    * [cite_start]객관식: 15문항 (각 1.5점) [cite: 11]
    * [cite_start]주관식(단답형): 3문항 (각 3점) [cite: 12]
    * [cite_start]서술형: 2문항 (4.5점, 4점) [cite: 13]
* [cite_start]**평가:** 분반별 평가 [cite: 15]
* **출제 범위**
    * [cite_start]현재 PPT에 있는 내용 범위에서 출제 [cite: 17]
    * 실습: 소스 코드 작성 문제는 없음. [cite_start]**소스 코드 실행 후 결과에 대한 해석** 위주의 문제. [cite: 18]

---

### 🤖 2. 강화학습(RL) 기본 개념

* [cite_start]**강화학습 정의:** 컴퓨터(에이전트) 스스로 **행동의 방식**을 익히는 기계학습. [cite: 27] [cite_start]**행동 심리학**에서 시작됨. [cite: 28]
* [cite_start]**머신러닝의 3가지 유형** [cite: 29]
    * [cite_start]**지도 학습:** 문제와 정답을 모두 알려주고 공부시키는 방법 (예: 예측, 분류) [cite: 30, 35, 36, 47]
    * [cite_start]**비지도 학습:** 답을 가르쳐주지 않고 공부시키는 방법 (예: 연관 규칙, 군집) [cite: 31, 37, 38, 49]
    * [cite_start]**강화 학습:** **보상**을 통해 상은 최대화하고 벌은 최소화하는 방향으로 행위를 강화하는 학습 (예: 최적 행동) [cite: 40, 41, 42, 50]
* **AI의 분류**
    * [cite_start]**약 인공지능:** 특정 일부 기술에만 적용 [cite: 43]
    * [cite_start]**강 인공지능:** 모든 분야에 일반적으로 적용 (강화학습은 강 인공지능을 위한 주요 기술) [cite: 34, 44]
    * [cite_start]**초 인공지능:** 인간을 뛰어넘음 [cite: 45]
* **심층 강화학습 (Deep Reinforcement Learning)**
    * [cite_start]**딥러닝 + 강화학습** [cite: 69, 70]
    * [cite_start]알고리즘의 성능, 학습 속도, 안정성 등 향상 [cite: 71]
    * [cite_start]딥마인드의 **DQN (Deep Q-Network)** 기술이 대표적 (예: 2016년 알파고, 2019년 알파스타) [cite: 75, 76]

### 🏛️ 3. 강화학습의 역사

* [cite_start]**스키너 상자 (1931년):** 강화학습의 **모티브**. [cite: 82] [cite_start]동물이 특정 행동(지렛대 누르기)을 했을 때 **보상(음식)**이나 **처벌(전기)**을 주어 조건 반응을 연구한 도구. [cite: 85, 86, 87, 88]
* [cite_start]**동적 프로그래밍 (DP, 1950년):** 강화학습의 **근간**이 되는 알고리즘. [cite: 98] [cite_start]리처드 벨만(Richard Bellman)이 창시. [cite: 99, 100]
* [cite_start]**시간차 학습 (TD Learning, 1980년대):** Richard Sutton이 제안. [cite: 117, 118]
* [cite_start]**Q-Learning (1989년):** Chris Watkins가 제안한 강화학습의 가장 유명한 알고리즘. [cite: 119]

---

### 🧩 4. 강화학습의 기본 요소

* [cite_start]**에이전트 (Agent):** 강화학습을 실행하는 주체, 행동하는 주체 (예: 로봇, 게임 캐릭터). [cite: 132]
* [cite_start]**환경 (Environment):** 에이전트가 상호작용하는 대상. [cite: 132]
* [cite_start]**상태 (State):** 에이전트가 환경으로부터 받는 관찰 정보. [cite: 132]
* [cite_start]**행동 (Action):** 에이전트가 환경에 전달하는 입력 정보. [cite: 140, 141]
* [cite_start]**보상 (Reward):** 에이전트의 행동에 대해 환경이 에이전트에게 전달하는 값. [cite: 140, 141]
* [cite_start]**강화학습의 목적:** **누적 보상의 기대값을 최대화**하는 것. [cite: 142, 143]
    * (보상 가설) [cite_start]나중의 더 큰 보상을 위해 즉각적인 보상을 희생할 수 있어야 함. [cite: 144]

#### 환경의 종류

1.  **종료 상태 유무 기준**
    * [cite_start]**에피소딕 환경 (Episodic):** 종료 상태가 존재 (예: 게임 한 판). [cite: 135]
    * [cite_start]**지속적 환경 (Continuing):** 종료 상태가 없음 (예: 주식 가격). [cite: 135]
2.  **결과 일정성 유무 기준**
    * [cite_start]**결정적 환경 (Deterministic):** 행동의 결과(다음 상태, 보상)가 항상 일정함. [cite: 137]
    * [cite_start]**확률적 환경 (Stochastic):** 행동의 결과가 매번 가변적임. [cite: 137]

---

### 🧭 5. 핵심 개념: 정책, 가치, 마르코프

* [cite_start]**정책 (Policy, $\pi$):** 에이전트가 주어진 상태에서 어떤 행동을 할지 결정하는 전략. [cite: 147, 148]
    * [cite_start]**결정적 정책:** 상태마다 행동이 1개로 정해짐. [cite: 148]
    * [cite_start]**확률적 정책:** 상태마다 행동을 할 확률이 정해짐. [cite: 148]
* **가치 함수 (Value Function):**
    * [cite_start]**상태 가치 함수 ($v(s)$):** 현재 *상태* $s$가 얼마나 좋은지 나타냄. [cite: 149, 150]
    * [cite_start]**행동 가치 함수 ($q(s,a)$):** 현재 상태 $s$에서 행동 $a$를 하는 것이 얼마나 좋은지 나타냄. [cite: 149, 150]
* [cite_start]**이득 (Return, $G_t$):** 타임 스텝 $t$ 이후부터 에피소드 종료까지의 보상 합. [cite: 152, 153]
    * **감가 이득 (Discounted Return):** 미래의 보상보다 현재의 보상을 더 가치 있게 계산. [cite_start]감가율($\gamma$)을 사용. [cite: 153]
        * $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$
        * [cite_start]$\gamma = 0$ 이면 당장의 보상($R_{t+1}$)만 고려 (근시안적). [cite: 190]
        * [cite_start]$\gamma = 1$ 이면 모든 미래 보상을 동등하게 고려. [cite: 190]
* **마르코프(Markov) 속성:**
    > [cite_start]"미래는 오직 현재에만 의존한다." [cite: 160, 161]

    * [cite_start]미래 상태는 과거의 모든 상태와 관계없이, **오직 현재 상태 $S_t$에만 의존**하여 결정된다는 가정. [cite: 161]
    * [cite_start]**마르코프 과정 (MP):** $\langle S, p \rangle$ (상태 집합, 상태 전이 확률) [cite: 178]
    * [cite_start]**마르코프 보상 과정 (MRP):** MP + 보상($r$), 감가율($\gamma$) $\rightarrow \langle S, p, r, \gamma, \mu \rangle$ [cite: 185, 186]
    * **마르코프 결정 과정 (MDP):** MRP + 행동($A$) $\rightarrow \langle S, A, p, r, \gamma \rangle$. [cite_start]강화학습 문제는 대부분 MDP로 표현됨. [cite: 191, 193]

---

### 💡 6. 문제 해결 방법론 (가치 추정)

[cite_start]강화학습의 목표는 **최적 정책($\pi_*$)**을 찾는 것입니다. [cite: 208]

* **벨만 방정식 (Bellman Equation):**
    * [cite_start]현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 나타내는 **재귀 방정식**. [cite: 205]
    * [cite_start]**한계:** 이 방정식을 풀려면 환경의 동적 특성(상태 전이 확률 $p$, 보상 $r$)을 정확히 알아야 함. [cite: 210, 211]

환경 모델(동적 특성)을 모를 때 사용하는 대표적인 두 가지 방법이 **몬테카를로(MC)**와 **시간차(TD)** 학습입니다.

#### 1. 몬테카를로 방법 (MC)

* [cite_start]**개념:** 수많은 반복 시행(시뮬레이션)을 통해 값을 확률적으로 근사. [cite: 226]
* **특징:**
    * [cite_start]환경의 완벽한 지식(**모델**)이 **필요 없음**. [cite: 227]
    * [cite_start]**에피소드가 끝난 후**에만 가치 함수를 갱신. [cite: 227]
    * [cite_start]종료 상태가 존재하는 **에피소딕 환경**에서만 적용 가능. [cite: 227]
    * [cite_start]**부트스트랩(Bootstrapping)을 사용하지 않음** (가치 추정 시 다른 상태의 가치 정보를 활용하지 않음). [cite: 237]
* **종류:**
    * [cite_start]**첫 방문(First-Visit) MC:** 한 에피소드에서 상태 $s$를 *처음* 방문했을 때의 이득만 평균. [cite: 231, 234]
    * [cite_start]**모든 방문(Every-Visit) MC:** 상태 $s$를 방문할 *때마다*의 이득을 모두 평균. [cite: 231, 233]

#### 2. 시간차 학습 (TD)

* [cite_start]**개념:** **몬테카를로(MC) + 동적 프로그래밍(DP)**의 장점을 결합. [cite: 243]
* **특징:**
    * [cite_start]MC처럼 환경 모델 없이 경험 샘플로부터 학습. [cite: 243]
    * [cite_start]DP처럼 **매 타임 스텝마다** 가치 함수를 갱신 (에피소드가 끝나길 기다리지 않음). [cite: 243]
    * [cite_start]**부트스트랩(Bootstrapping) 사용:** 임의의 상태 가치를 추정할 때, 다른 상태의 가치 정보를 활용. [cite: 243]
* **TD의 장점 (vs. MC):**
    * [cite_start]**지속적 환경**(종료 없음)에도 적용 가능. [cite: 248]
    * [cite_start]에피소드 도중에 **실시간으로 학습** 가능 (학습 시간이 더 빠름). [cite: 248]
    * [cite_start]실험적(경험적)으로 TD가 MC보다 더 빠르게 학습하는 것으로 밝혀짐. [cite: 249]

#### [cite_start]3. MC vs. TD vs. DP 비교 (중요!) [cite: 250]

| 비교 항목 | 동적 프로그래밍 (DP) | 몬테카를로 (MC) | 시간차 (TD) |
| :--- | :--- | :--- | :--- |
| **환경 모델 필요 여부** | **O** (필수) | **X** (불필요) | **X** (불필요) |
| **갱신 시점** | (모델 기반) | 에피소드 **종료** 후 | 매 **타임 스텝**마다 |
| **부트스트랩 사용** | **O** | **X** | **O** |
| **적용 환경** | - | 에피소딕 | 에피소딕 + 지속적 |

* [cite_start]**배치 업데이트 (Batch Update):** 여러 에피소드 경험 샘플(배치)을 모아 가치 함수를 한 번에 갱신하는 방식. [cite: 251, 252] [cite_start]동일 샘플을 반복적으로 활용하고 부트스트랩을 이용하므로 더 정확한 가치 추정이 가능. [cite: 256]

---

### [cite_start]📚 7. 강화학습 알고리즘 분류 (Page 26) [cite: 164]

* **가치 기반 (Value-Based):**
    * [cite_start]가치 함수를 구성하고 활용 (정책은 명시적으로 존재하지 않음). [cite: 165]
    * [cite_start]대표 알고리즘: **DQN** [cite: 165]
* **정책 기반 (Policy-Based):**
    * [cite_start]별도의 가치 함수 없이, 정책 그 자체를 직접 구성하고 최적화. [cite: 165]
    * [cite_start]대표 알고리즘: **REINFORCE** [cite: 165]
* **액터-크리틱 (Actor-Critic):**
    * [cite_start]가치 함수(Critic)와 정책(Actor) 둘 다 명시적으로 구성하고 활용. [cite: 165]
    * [cite_start]대표 알고리즘: **A3C** [cite: 165]

이 요약본이 시험공부에 도움이 되기를 바랍니다. 혹시 특정 개념(예: 벨만 방정식, MC와 TD의 차이)에 대해 더 자세한 설명이 필요하신가요?
