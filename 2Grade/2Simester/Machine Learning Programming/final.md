## 2. SARSA와 Q-Learning 비교

SARSA와 Q-Learning은 모두 시간차(TD) 예측 알고리즘에 속합니다.

---

### **구분별 비교**

| 구분 | SARSA | Q-Learning |
|------|--------|-------------|
| **정책 분류** | On-policy (행동 정책 = 학습 정책) | Off-policy (행동 정책 ≠ 학습 정책) |
| **행동 선택** | 현재 상태 \(S_t\)와 다음 상태 \(S_{t+1}\) 모두에서 ε-greedy 정책 사용 | 현재 상태 \(S_t\)는 ε-greedy, 다음 상태 \(S_{t+1}\)는 최대 Q값 \(\max Q\) 사용 |
| **Q 함수 갱신식** | \( Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s,a)) \) | \( Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma \max_{a'}Q(s_{t+1}, a') - Q(s,a)) \) |
| **특징** | 실제 행동 궤적(trajectory)을 따라 가치 업데이트 | 다음 행동은 항상 최적 행동(가장 큰 Q값) 기준으로 학습 |

---

### **SARSA의 문제점과 Q-Learning의 장점**

- **SARSA 문제점**  
  ε-greedy 탐험 과정에서 잘못된 행동(예: 절벽으로 떨어짐)이 발생하면 경로 가치가 불필요하게 낮아져 학습이 느려지거나 갇히는 현상이 발생할 수 있음.

- **Q-Learning 장점**  
  다음 행동을 항상 최적의 행동으로 평가하기 때문에 잘못된 탐험의 영향을 덜 받고, 시간이 지날수록 정책이 발전하면 같은 행동도 다시 더 정확히 평가됨.

---

---

## 3. Expected SARSA (기대값 기반 SARSA)

- **특징:** 정해진 하나의 행동을 쓰는 대신, 가능한 모든 행동의 기대값을 반영하여 갱신 → SARSA의 높은 분산을 완화  
- **정책:** On-policy  
- **갱신:**  
  다음 상태에서의 행동 분포 전체에 대한 기대 Q값 사용

---

## 4. 모델 기반 학습 vs. 모델 프리 학습

| 구분 | 계획 (Planning) — 모델 기반 | 학습 (Learning) — 모델 프리 |
|------|-----------------------------|------------------------------|
| **목적** | 목표 달성 방법·절차 준비 | 환경과 상호작용하며 방법 탐색 |
| **활용** | 환경 모델에게 질문하며 의사결정 | 환경 경험 데이터를 통해 학습 |
| **예시** | 동적 프로그래밍, Dyna-Q | SARSA, Q-Learning |

---

### **Dyna-Q (온라인 계획)**

- **특징:** 계획(모델 기반) + 직접 학습(모델 프리)를 동시에 수행하는 대표 알고리즘  
- **갱신 경로:**  
  1) 모델 학습 및 계획(간접 강화 학습)  
  2) 직접 환경에서 얻은 경험 기반 갱신  
- **장점:** 환경과의 상호작용이 적어도 기존 경험을 여러 번 재사용해 높은 샘플 효율성을 가짐  
- **실험 결과:** planning 반복 횟수(planning_repeats)가 많을수록 더 적은 에피소드만으로 높은 성능 달성  

---

## 5. 기타 강화 학습 기법

- **기대 갱신(Expected Update)**  
  가능한 모든 상황을 고려하여 Q값을 갱신  
- **샘플 갱신(Sample Update)**  
  단일 샘플만 사용하여 Q값을 갱신 (예: Q-Learning)
- **On-policy 분포 샘플링**  
  현재 정책대로 행동을 선택하며 실제 발생 가능성이 높은 상황을 반복적으로 학습 → 학습 효율 향상  

